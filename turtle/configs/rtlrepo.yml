benchmark:
  - task: RTLRepo
    path_dataset_test: /your_path
    rtlrepo_use_train_ds: False
    path_model: /your_path
    metric_output_path: /your_path
    singularity_image: /your_path
    models:
      - name: CodeLlama-70b-hf
        multinode: True
        slurm_config: general-multi-node
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 17
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: CodeV-CL-7B
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 64
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: CodeV-DS-6.7B
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 64
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: CodeV-QW-7B
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        swap_space: 50
        continuous_batching_size: 64
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: DeepSeek-R1
        multinode: True
        slurm_config: general-multi-node
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        swap_space: 256
        continuous_batching_size: 17
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
        prompt: r1
      - name: deepseek-coder-6.7b-base
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 64
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: deepseek-coder-33b-base
        multinode: False
        slurm_config: general-single-node-large
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        swap_space: 50
        continuous_batching_size: 16
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: DeepSeek-R1-Distill-Qwen-14B
        multinode: False
        slurm_config: general-single-node-large
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 64
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: Origen_merged
        multinode: False
        slurm_config: general-single-node-large
        tensor_parallel_size: 1
        max_length_generation: 4096
        gpu_memory_utilization: 0.95
        swap_space: 50
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 64
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: HaVen-CodeQwen
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: OpenCoder-8B-Base
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: Qwen2.5-32B
        multinode: False
        slurm_config: general-single-node-large
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        swap_space: 50
        continuous_batching_size: 16
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: Qwen2.5-72B
        multinode: True
        slurm_config: general-multi-node
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        swap_space: 256
        continuous_batching_size: 17
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: Qwen2.5-Coder-7B
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 64
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: Qwen2.5-Coder-14B
        multinode: False
        slurm_config: general-single-node-large
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 64
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: Qwen2.5-Coder-32B
        multinode: False
        slurm_config: general-single-node-large
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        swap_space: 50
        continuous_batching_size: 16
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: RTLCoder-Deepseek-v1.1
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 32
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: RTLCoder-Mistral-v1.1
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 32
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: starchat2-15b-v0.1
        multinode: False
        slurm_config: general-single-node-small
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 32
        precision: fp16
        gpu_memory_utilization: 0.9
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: gemma-3-27b-pt
        slurm_config: general-single-node-large
        max_length_generation: 1024
        temperature: [0.2]
        n_samples: 5
        swap_space: 50
        continuous_batching_size: 10
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: gemma-3-27b-it
        slurm_config: general-single-node-large
        max_length_generation: 1024
        temperature: [0.2]
        n_samples: 5
        swap_space: 50
        continuous_batching_size: 10
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: Seed-Coder-8B-Base
        slurm_config: general-single-node-large
        temperature: [0.2]
        max_length_generation: 8192
        max_tokens: 50
        n_samples: 5
        swap_space: 50
        continuous_batching_size: 32
        few_shot: 0
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: CodeLlama-70b-hf
        slurm_config: general-multi-node
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 32
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: DeepSeek-R1
        slurm_config: general-multi-node
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 17
        swap_space: 50
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
        prompt: r1
      - name: Meta-Llama-3.1-70B
        slurm_config: general-multi-node
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 32
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: neuralmagic_Meta-Llama-3.1-405B-FP8
        slurm_config: general-multi-node
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        swap_space: 50
        continuous_batching_size: 17
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: Qwen2.5-72B
        slurm_config: general-multi-node
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        continuous_batching_size: 32
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
      - name: Qwen3-235B-A22B
        slurm_config: general-multi-node
        max_length_generation: 8192
        max_tokens: 50
        temperature: [0.2]
        n_samples: 5
        swap_space: 50
        continuous_batching_size: 32
        precision: fp16
        save_generations: True
        save_metrics: True
        save_references: False
        trust_remote_code: True
